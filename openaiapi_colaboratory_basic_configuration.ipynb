{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9cQxD0k+IOZ1HR53dHm9V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredoaguiararce/OpenAiColaboratoryEnvironment/blob/main/openaiapi_colaboratory_basic_configuration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Download the dependncies**\n",
        "\n",
        "* The first line installs the \"openai\" package, which is necessary for working with OpenAI's language models and APIs.\n",
        "* The second line installs the \"python-dotenv\" package, which is useful for loading environment variables from a .env file, providing a secure way to store sensitive information such as API keys.\n",
        "\n"
      ],
      "metadata": {
        "id": "79guol5N3i1C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4TtI-v5-aV6",
        "outputId": "c295052f-01c4-4dda-96ea-38778e39d84f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup your .env file\n",
        "\n",
        "To add a **`.env`** file to Colaboratory, follow these steps:\n",
        "\n",
        "1. In your Colaboratory notebook, click on the \"Files\" tab on the left-hand side of the interface.\n",
        "2. Click on the \"New\" button and select \"Text File\" to create a new text file.\n",
        "3. Rename the file to **`.env`** (including the leading dot) and press Enter to save the file name.\n",
        "4. Open the **`.env`** file by double-clicking on it. It will open in an editor within Colaboratory.\n",
        "5. Add your environment variables in the **`.env`** file, following the format of **`KEY=VALUE`**. For example:\n",
        "6. Save the changes to the **`.env`** file.\n",
        "\n",
        "Example of **`.env`** file\n",
        "\n",
        "```bash\n",
        "OPENAI_API_KEY=your-api-key\n",
        "```\n",
        "\n",
        "To access the environment variables from the **`.env`** file in your Colaboratory notebook, you can use the **`dotenv`** package. Import the **`dotenv`** module and use the **`load_dotenv()`** function to load the environment variables from the **`.env`** file."
      ],
      "metadata": {
        "id": "2nihTQC97_8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "In the provided code snippet, the openai and os modules are imported to access their respective functionalities. The dotenv module is imported from the dotenv package to load environment variables from a .env file.\n",
        "\n",
        "> The line _ = load_dotenv(find_dotenv()) is used to load the environment variables from the .env file located in the project directory. This step ensures that sensitive information, such as the OpenAI API key, can be securely stored and accessed.\n",
        "\n",
        "Finally, the OpenAI API key is assigned to openai.api_key by retrieving the value from the environment variable named 'OPENAI_API_KEY', which is stored in the .env file. This allows the code to authenticate and access the OpenAI services using the provided API key."
      ],
      "metadata": {
        "id": "1uJ0D6mV-kK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "# You can use other name defined in the .env file\n",
        "key_name = 'OPENAI_API_KEY'\n",
        "openai.api_key  = os.getenv(key_name)"
      ],
      "metadata": {
        "id": "caXbWLem-l-l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic prompt function\n",
        "\n",
        "The **`get_completion`** function takes a **`prompt`** as input and generates a completion using the OpenAI library. It uses the provided prompt, model, and temperature to create a chat-based completion. The function returns the content of the completion generated.\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "| Parameter | Description |\n",
        "| --- | --- |\n",
        "| prompt | The input prompt for which the completion is generated. |\n",
        "| model | The OpenAI model to use for generating the completion. Defaults to \"gpt-3.5-turbo-16k-0613\". |\n",
        "| temperature | The temperature parameter controls the randomness of the completion. A higher value (e.g., 0.8) makes the output more random, while a lower value (e.g., 0.2) makes it more focused and deterministic. Defaults to 0. |\n",
        "\n",
        "\n",
        "\n",
        "> üí° By specifying the prompt, model, and temperature, you can fine-tune the behavior of the completion generation according to your requirements."
      ],
      "metadata": {
        "id": "J4qup6d64TGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo-16k-0613\", temperature=0):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "ALmzGApg-pw4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Notes\n",
        "OpenAI enforces rate limits on API requests, including requests-per-minute (RPM), tokens-per-minute (TPM), or images-per-minute, to ensure fair usage and system stability.\n",
        "\n",
        "* Requests per minute (RPM): RPM refers to the maximum number of API requests you can make to the OpenAI API within a span of one minute. Each API call, such as generating a completion or making a chat-based request, counts as one request towards the RPM limit.\n",
        "\n",
        "* Tokens per minute (TPM): TPM represents the maximum number of tokens that can be processed by the API within a minute. Tokens are units of text used by OpenAI models, where each word or character is considered a token. Longer prompts or responses consume more tokens. The total token count includes both input and output tokens.\n",
        "\n",
        "Detailed rate limit information, including default limits for each model, can be found in OpenAI's documentation. Organizations may have different rate limits based on their subscription or plan, which can be checked in the OpenAI account dashboard. For more information, visit the provided link."
      ],
      "metadata": {
        "id": "ciqgwXd-6Jmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü™Å Basic example with Translation function"
      ],
      "metadata": {
        "id": "nO3CXzoT6zMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation\n",
        "\n",
        "ChatGPT is trained with sources in many languages. This gives the model the ability to do translation. Here are some examples of how to use this capability."
      ],
      "metadata": {
        "id": "z5SL-Zcd-sSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Translate the following English text to Spanish: \\\n",
        "```Hi, I would like to order a blender```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V1YNZb8-txT",
        "outputId": "62d24ab5-4543-4e0b-87a6-78afb4dea231"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola, me gustar√≠a ordenar una licuadora.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Tell me which language this is:\n",
        "```Combien co√ªte le lampadaire?```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp2aGjz0-xIn",
        "outputId": "dbf4e3b8-febe-4306-f714-943a1f6c3b9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This language is French.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Translate the following  text to French and Spanish\n",
        "and English pirate: \\\n",
        "```I want to order a basketball```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIXOb3qP-yfU",
        "outputId": "a8a0d15d-c88c-44bb-d901-4a5b696c6ec7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French: ```Je veux commander un ballon de basket```\n",
            "Spanish: ```Quiero ordenar una pelota de baloncesto```\n",
            "English: ```I want to order a basketball```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use your prompts"
      ],
      "metadata": {
        "id": "I5B20jLe8x-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"this is my prompt\n",
        "        \"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ4dpubq85ZG",
        "outputId": "7b2cd3b4-f3cb-43bc-aae3-46402bc02ffd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but you haven't provided a prompt. Could you please provide more information or a specific prompt for me to assist you with?\n"
          ]
        }
      ]
    }
  ]
}